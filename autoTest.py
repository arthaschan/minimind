import json
import torch
import os
import sys
import argparse

# åŠ å…¥é¡¹ç›®æ ¹ç›®å½•å’Œmodelç›®å½•åˆ°Pythonè·¯å¾„ï¼ˆé€‚é…ä»£ç ä»“ç»“æ„ï¼‰
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), "model"))

# å¤ç”¨eval_llm_medical.pyä¸­ç”¨åˆ°çš„æ ¸å¿ƒå¯¼å…¥ï¼ˆä¸ä»£ç ä»“ä¿æŒä¸€è‡´ï¼‰
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer
from model.model_minimind import MiniMindConfig, MiniMindForCausalLM
from model.model_lora import *
from trainer.trainer_utils import setup_seed
# ===================== åŸºç¡€é…ç½®ï¼ˆä¸eval_llm_medical.pyå¯¹é½ï¼Œé€‚é…ä»£ç ä»“ï¼‰ =====================
DEFAULT_CONFIG = {
    "model_path": "./model",  # æ¨¡å‹ç»“æ„/åˆ†è¯å™¨é…ç½®ç›®å½•
    "checkpoint_path": "./out/full_sft_512.pth",  # å¾®è°ƒæƒé‡æ–‡ä»¶
    "lora_checkpoint_path": "./out/lora/lora_medical_mental_512.pth",  # LoRAæƒé‡ï¼ˆå¯é€‰ï¼‰
    "test_data_path": "./dataset/mental.jsonl",
    "output_report_path": "dental_model_test_report.json",
    "max_seq_len": 1024,
    "batch_size": 1,
    "seed": 42,
    "device": "cuda" if torch.cuda.is_available() else "cpu"
}

 
def init_model(args):
    tokenizer = AutoTokenizer.from_pretrained(args.load_from)
    if 'model' in args.load_from:
        model = MiniMindForCausalLM(MiniMindConfig(
            hidden_size=args.hidden_size,
            num_hidden_layers=args.num_hidden_layers,
            use_moe=bool(args.use_moe),
            inference_rope_scaling=args.inference_rope_scaling
        ))
        moe_suffix = '_moe' if args.use_moe else ''
        ckp = f'./{args.save_dir}/{args.weight}_{args.hidden_size}{moe_suffix}.pth'
        model.load_state_dict(torch.load(ckp, map_location=args.device), strict=True)
        if args.lora_weight != 'None':
            apply_lora(model)
            load_lora(model, f'./{args.save_dir}/lora/{args.lora_weight}_{args.hidden_size}.pth')
    else:
        model = AutoModelForCausalLM.from_pretrained(args.load_from, trust_remote_code=True)
    print(f'MiniMindæ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()) / 1e6:.2f} M(illion)')
    return model.eval().to(args.device), tokenizer


def build_choice_prompt(question: str, options: str | dict) -> str:
    """
    æ„å»ºæ ‡å‡†åŒ–é€‰æ‹©é¢˜Promptï¼Œå…¼å®¹dictå’Œå­—ç¬¦ä¸²æ ¼å¼çš„é€‰é¡¹ï¼Œæ”¯æŒA-Eé€‰é¡¹
    :param question: é€‰æ‹©é¢˜é¢˜å¹²
    :param options: é€‰é¡¹ï¼ˆdictæ ¼å¼ï¼š{"A": "xxx"} æˆ– å­—ç¬¦ä¸²æ ¼å¼ï¼š"A xxx\nB xxx"ï¼‰
    :return: æ ¼å¼åŒ–åçš„Prompt
    """
    # æ ¸å¿ƒï¼šç»Ÿä¸€å°†optionsè½¬ä¸ºæ ‡å‡†çš„ã€Œå­—æ¯ï¼šå†…å®¹ã€æ ¼å¼å­—ç¬¦ä¸²
    standard_options = ""
    
    # åœºæ™¯1ï¼šoptionsæ˜¯dictï¼ˆé”®ä¸ºA/B/C/D/Eï¼Œå€¼ä¸ºé€‰é¡¹å†…å®¹ï¼‰
    if isinstance(options, dict):
        for opt_letter, opt_content in sorted(options.items()):  # sortedä¿è¯A-Eé¡ºåºä¸ä¹±
            standard_options += f"{opt_letter}ï¼š{opt_content.strip()}\n"
    
    # åœºæ™¯2ï¼šoptionsæ˜¯å­—ç¬¦ä¸²ï¼ˆå¦‚ä½ ç»™å‡ºçš„åŸå§‹æ ¼å¼ï¼‰
    elif isinstance(options, str):
        # æŒ‰è¡Œæ‹†åˆ†å­—ç¬¦ä¸²ï¼Œé€è¡Œå¤„ç†
        option_lines = options.strip().split("\n")
        for line in option_lines:
            line = line.strip()
            if not line:
                continue
            # æå–é€‰é¡¹å­—æ¯ï¼ˆå¼€å¤´ç¬¬ä¸€ä¸ªå­—ç¬¦ï¼Œå¦‚A/B/C/Eï¼‰
            opt_letter = line[0].upper()
            # æå–é€‰é¡¹å†…å®¹ï¼ˆå»é™¤å­—æ¯åçš„éƒ¨åˆ†ï¼Œå¤„ç†ç©ºæ ¼/é¡¿å·ï¼‰
            opt_content = line[1:].strip().lstrip("ï¼š").lstrip(".").lstrip(" ").strip()
            standard_options += f"{opt_letter}ï¼š{opt_content}\n"
    
    # æ„å»ºæœ€ç»ˆPromptï¼Œä¼˜åŒ–å¼•å¯¼è¯­ï¼ˆæ˜ç¡®æ”¯æŒA-Eï¼‰
    prompt = f"""è¯·å›ç­”ä»¥ä¸‹é€‰æ‹©é¢˜ï¼Œä»…éœ€è¾“å‡ºæ­£ç¡®é€‰é¡¹çš„å­—æ¯ï¼ˆå¦‚Aã€Bã€Cã€Dã€Eï¼‰ï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ï¼Œæ— éœ€é¢å¤–è§£é‡Šã€‚
    é—®é¢˜ï¼š{question.strip()}
    é€‰é¡¹ï¼š
    {standard_options.strip()}

    ç­”æ¡ˆï¼š"""
    
    return prompt.strip()

# ===================== å¤ç”¨eval_llm_medical.pyæ¨ç†é€»è¾‘ï¼šè·å–æ¨¡å‹å›ç­” =====================
@torch.no_grad()
def get_model_answer(model, tokenizer, prompt, config):
    """
    å¤ç”¨eval_llm_medical.pyçš„è´ªå¿ƒæ¨ç†é€»è¾‘ï¼Œé€‚é…è‡ªå®šä¹‰æ¨¡å‹å’ŒBPE Tokenizer
    """
    # 1. BPEç¼–ç ï¼ˆä¸eval_llm_medical.pyä¸€è‡´ï¼Œä½¿ç”¨è‡ªå®šä¹‰tokenizerçš„encodeï¼‰
    input_ids = tokenizer.encode(
        prompt,
        add_special_tokens=True,
        truncation=True,
        max_length=config["max_seq_len"]
    )
    input_ids = torch.tensor([input_ids], dtype=torch.long).to(config["device"])
    attention_mask = torch.ones_like(input_ids).to(config["device"])

    # 2. è´ªå¿ƒç”Ÿæˆï¼ˆä»…ç”Ÿæˆ1-2ä¸ªå­—ç¬¦ï¼Œç¡®ä¿åªè¿”å›é€‰é¡¹å­—æ¯ï¼Œä¸eval_llm_medical.pyæ¨ç†é€»è¾‘å¯¹é½ï¼‰
    generated_ids = input_ids
    max_new_tokens = 2  # é˜²æ­¢æˆªæ–­ï¼Œå…¼å®¹è½»å¾®å†—ä½™è¾“å‡º
    for _ in range(max_new_tokens):
        # å‰å‘ä¼ æ’­ï¼ˆå¤ç”¨æ¨¡å‹çš„forwardé€»è¾‘ï¼Œä¸eval_llm_medical.pyä¸€è‡´ï¼‰
        outputs = model(
            input_ids=generated_ids,
            attention_mask=attention_mask,
            labels=None
        )
        logits = outputs.logits if hasattr(outputs, "logits") else outputs[0]

        # å–æœ€åä¸€ä¸ªtokençš„logitsï¼Œè´ªå¿ƒè§£ç 
        next_token_logits = logits[:, -1, :]
        next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)

        # ç»ˆæ­¢æ¡ä»¶ï¼šç”Ÿæˆeos tokenåˆ™åœæ­¢
        if next_token_id.item() == tokenizer.eos_token_id:
            break

        # æ‹¼æ¥ç”Ÿæˆçš„token
        generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)
        attention_mask = torch.cat([
            attention_mask,
            torch.ones((1, 1), dtype=torch.long, device=config["device"])
        ], dim=-1)

    # 3. BPEè§£ç ï¼ˆä¸eval_llm_medical.pyä¸€è‡´ï¼Œè·³è¿‡ç‰¹æ®Štokenï¼‰
    response = tokenizer.decode(
        generated_ids[0].cpu().numpy(),
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False  # ä¿ç•™BPEè§£ç åŸå§‹ç»“æœï¼Œé¿å…å­—æ¯é”™ä¹±
    )

    # 4. æå–æœ‰æ•ˆç­”æ¡ˆï¼ˆä»…ä¿ç•™A/B/C/Dï¼‰
    answer_part = response.split("ç­”æ¡ˆï¼š")[-1].strip().upper()
    model_answer = ""
    for char in answer_part:
        if char in ["A", "B", "C", "D","E"]:
            model_answer = char
            break
    return model_answer if model_answer else "æœªçŸ¥"

 # ===================== 2. åŠ è½½æµ‹è¯•æ•°æ®ï¼ˆé€‚é… jsonl æ–‡ä»¶ï¼‰ =====================
 # æ–‡ä»¶æ˜¯å¯¹è¯èŠå¤©æ ¼å¼
def batch_extract_qa_from_jsonl(jsonl_path: str, output_path: str = "extracted_qa.jsonl"):
    """
    æ‰¹é‡å¤„ç†JSONLæ–‡ä»¶ï¼Œæå–æ¯ä¸€è¡Œçš„questionå’Œanswerå¹¶ä¿å­˜
    """
    with open(jsonl_path, "r", encoding="utf-8") as in_f, open(output_path, "w", encoding="utf-8") as out_f:
        for line_num, line in enumerate(in_f, 1):
            line = line.strip()
            if not line:
                continue
            qa_result = extract_qa_from_conversation(line)
            # ä¿å­˜æ‰¹é‡æå–ç»“æœ
            out_f.write(json.dumps(qa_result, ensure_ascii=False) + "\n")
    print(f"âœ… æ‰¹é‡æå–å®Œæˆï¼Œç»“æœä¿å­˜è‡³ï¼š{output_path}")

# è°ƒç”¨æ‰¹é‡å¤„ç†å‡½æ•°
batch_extract_qa_from_jsonl("./sft_mini_512_with_choice_train.jsonl")

 # ===================== 2. åŠ è½½æµ‹è¯•æ•°æ®ï¼ˆé€‚é… jsonl æ–‡ä»¶ï¼‰ =====================

 # æ–‡ä»¶æ˜¯cmexam é‡Œçš„Questionã€‚Answer è¿™ç§æ ¼å¼
def jsonload(jsonl_path):
    if not os.path.exists(config["test_data_path"]):
        raise FileNotFoundError(f"æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{config['test_data_path']}")

    # åˆå§‹åŒ–æµ‹è¯•é¢˜ç›®åˆ—è¡¨
    test_questions = []
    with open(jsonl_path, "r", encoding="utf-8") as f:
        # é€è¡Œè¯»å– jsonl æ–‡ä»¶
        for line_num, line in enumerate(f, 1):
            # å»é™¤è¡Œé¦–å°¾ç©ºç™½å­—ç¬¦ï¼ˆç©ºæ ¼ã€æ¢è¡Œã€åˆ¶è¡¨ç¬¦ç­‰ï¼‰
            line = line.strip()
            # è·³è¿‡ç©ºè¡Œ
            if not line:
                continue
            try:
                # è§£æå•è¡Œ JSON å¯¹è±¡
                question_obj = json.loads(line)
                # å°†è§£æåçš„å¯¹è±¡åŠ å…¥åˆ—è¡¨
                test_questions.append(question_obj)
            except json.JSONDecodeError as e:
                # æ•è·å•è¡Œè§£æé”™è¯¯ï¼Œç»™å‡ºå‹å¥½æç¤ºï¼Œä¸ä¸­æ–­æ•´ä½“åŠ è½½
                print(f"âš ï¸  ç¬¬ {line_num} è¡Œ JSON æ ¼å¼é”™è¯¯ï¼Œè·³è¿‡è¯¥è¡Œï¼š{e}")

    # éªŒè¯æ˜¯å¦åŠ è½½åˆ°æœ‰æ•ˆæ•°æ®
    if not test_questions:
    raise ValueError(f"jsonl æ–‡ä»¶ä¸­æ— æœ‰æ•ˆæµ‹è¯•æ•°æ®ï¼š{jsonl_path}")

    print(f"âœ… æµ‹è¯•æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(test_questions)} é“æœ‰æ•ˆé¢˜ç›®")
    return test_questions
# ===================== æ‰¹é‡æµ‹è¯•ï¼ˆä¿ç•™åŸä¸šåŠ¡é€»è¾‘ï¼Œé€‚é…æ–°çš„æ¨¡å‹/Tokenizerï¼‰ =====================
def run_batch_test(config,args):
    """
    æ‰§è¡Œæ‰¹é‡é€‰æ‹©é¢˜æµ‹è¯•ï¼Œç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    """
    # 1. åŠ è½½æ¨¡å‹å’ŒTokenizerï¼ˆå¤ç”¨éªŒè¯è¿‡çš„é€»è¾‘ï¼‰
    model, tokenizer = init_model(args)   
    test_questions = jsonload(config)
    # 3. åˆå§‹åŒ–æµ‹è¯•ç»“æœ
    test_results = {
        "total_questions": len(test_questions),
        "correct_count": 0,
        "incorrect_count": 0,
        "accuracy": 0.0,
        "detailed_results": []
    }

    # 4. éå†æ‰§è¡Œæµ‹è¯•
    for idx, q in enumerate(test_questions):
        # æå–é¢˜ç›®å­—æ®µï¼ˆå…¼å®¹jsonå­—æ®µï¼Œæé«˜å®¹é”™æ€§ï¼‰
        q_id = q.get("Question", idx + 1)
        question = q.get("Question", "")
        options = q.get("Options", {})
        correct_answer = q.get("Answer", "").upper()

        # è·³è¿‡æ— æ•ˆé¢˜ç›®
        if not question or not options or not correct_answer or correct_answer not in ["A", "B", "C", "D","E"]:
            print(f"âš ï¸  è·³è¿‡ç¬¬ {idx+1} é¢˜ï¼ˆIDï¼š{q_id}ï¼‰ï¼šå­—æ®µç¼ºå¤±æˆ–æ— æ•ˆ")
            test_results["incorrect_count"] += 1
            continue

        # æ‰“å°è¿›åº¦
        print(f"ğŸ“ æµ‹è¯•ç¬¬ {idx+1}/{len(test_questions)} é¢˜ï¼ˆIDï¼š{q_id}ï¼‰")

        # 5. æ„å»ºPromptå¹¶è·å–æ¨¡å‹å›ç­”
        prompt = build_choice_prompt(question, options)
        model_answer = get_model_answer(model, tokenizer, prompt, config)

        # 6. ç»Ÿè®¡ç»“æœ
        is_correct = (model_answer == correct_answer) and (model_answer != "æœªçŸ¥")
        if is_correct:
            test_results["correct_count"] += 1
        else:
            test_results["incorrect_count"] += 1

        # 7. è®°å½•è¯¦ç»†ç»“æœ
        test_results["detailed_results"].append({
            "question_id": q_id,
            "question": question,
            "options": options,
            "model_answer": model_answer,
            "correct_answer": correct_answer,
            "is_correct": is_correct,
        })

    # 8. è®¡ç®—å‡†ç¡®ç‡ï¼ˆé¿å…é™¤é›¶é”™è¯¯ï¼‰
    if test_results["total_questions"] > 0:
        test_results["accuracy"] = round(
            (test_results["correct_count"] / test_results["total_questions"]) * 100,
            2
        )

    # 9. ä¿å­˜æµ‹è¯•æŠ¥å‘Š
    with open(config["output_report_path"], "w", encoding="utf-8") as f:
        json.dump(test_results, f, ensure_ascii=False, indent=4)
    print(f"âœ… æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜è‡³ï¼š{config['output_report_path']}")

    return test_results

# ===================== ç”Ÿæˆäººç±»å‹å¥½å‹æŠ¥å‘Šï¼ˆä¿ç•™åŸé€»è¾‘ï¼Œä¿®å¤å­—æ®µé”™è¯¯ï¼‰ =====================
def generate_human_report(test_results, config):
    """
    ç”Ÿæˆæ˜“è¯»çš„æ–‡æœ¬æ ¼å¼æµ‹è¯•æŠ¥å‘Š
    """
    report_path = "dental_test_report.txt"
    report = f"""# ç‰™ç§‘èŠå¤©æœºå™¨äººé€‰æ‹©é¢˜æ€§èƒ½æµ‹è¯•æŠ¥å‘Š
## æµ‹è¯•æ¦‚å†µ
- æ€»æµ‹è¯•é¢˜ç›®æ•°ï¼š{test_results['total_questions']}
- æ­£ç¡®æ•°ï¼š{test_results['correct_count']}
- é”™è¯¯æ•°ï¼š{test_results['incorrect_count']}
- æ•´ä½“å‡†ç¡®ç‡ï¼š{test_results['accuracy']}%

## é”™è¯¯é¢˜ç›®è¯¦æƒ…
"""

    # æå–é”™è¯¯é¢˜ç›®
    wrong_questions = [r for r in test_results["detailed_results"] if not r["is_correct"]]
    if wrong_questions:
        for r in wrong_questions:
            report += f"""### é¢˜ç›®IDï¼š{r['question_id']}
é—®é¢˜ï¼š{r['question']}
é€‰é¡¹ï¼š
"""

            report += f"æ¨¡å‹å›ç­”ï¼š{r['model_answer']}\n"
            report += f"æ­£ç¡®ç­”æ¡ˆï¼š{r['correct_answer']}\n\n"
    else:
        report += "æ— é”™è¯¯é¢˜ç›®ï¼Œæ¨¡å‹å›ç­”å…¨éƒ¨æ­£ç¡®ï¼\n"

    # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(report)
    print(f"âœ… äººç±»å‹å¥½å‹æŠ¥å‘Šå·²ä¿å­˜è‡³ï¼š{report_path}")

# ===================== ä¸»å‡½æ•°ï¼ˆé€‚é…ä»£ç ä»“é£æ ¼ï¼Œæ”¯æŒå‘½ä»¤è¡Œå‚æ•°ï¼‰ =====================
if __name__ == "__main__":
    # æ„å»ºå‘½ä»¤è¡Œå‚æ•°ï¼ˆä¸eval_llm_medical.pyé£æ ¼ä¸€è‡´ï¼‰
    parser = argparse.ArgumentParser(description="ç‰™ç§‘æ¨¡å‹é€‰æ‹©é¢˜æ‰¹é‡æµ‹è¯•ï¼ˆå¤ç”¨eval_llm_medical.pyé€»è¾‘ï¼‰")
    parser.add_argument('--load_from', default='model', type=str, help="æ¨¡å‹åŠ è½½è·¯å¾„ï¼ˆmodel=åŸç”Ÿtorchæƒé‡ï¼Œå…¶ä»–è·¯å¾„=transformersæ ¼å¼ï¼‰")
    parser.add_argument("--model_path", type=str, default=DEFAULT_CONFIG["model_path"], help="æ¨¡å‹ç»“æ„ç›®å½•")
    parser.add_argument("--checkpoint_path", type=str, default=DEFAULT_CONFIG["checkpoint_path"], help="åŸºç¡€æƒé‡æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--lora_checkpoint_path", type=str, default=DEFAULT_CONFIG["lora_checkpoint_path"], help="LoRAæƒé‡æ–‡ä»¶è·¯å¾„")
   
   
    parser.add_argument('--save_dir', default='out', type=str, help="æ¨¡å‹æƒé‡ç›®å½•")
    parser.add_argument('--weight', default='full_sft', type=str, help="æƒé‡åç§°å‰ç¼€ï¼ˆpretrain, full_sft, rlhf, reason, ppo_actor, grpo, spoï¼‰")
    parser.add_argument('--lora_weight', default='lora_medical_mental', type=str, help="LoRAæƒé‡åç§°ï¼ˆNoneè¡¨ç¤ºä¸ä½¿ç”¨ï¼Œå¯é€‰ï¼šlora_identity, lora_medicalï¼‰")
    parser.add_argument('--hidden_size', default=512, type=int, help="éšè—å±‚ç»´åº¦ï¼ˆ512=Small-26M, 640=MoE-145M, 768=Base-104Mï¼‰")
    parser.add_argument('--num_hidden_layers', default=8, type=int, help="éšè—å±‚æ•°é‡ï¼ˆSmall/MoE=8, Base=16ï¼‰")
    parser.add_argument('--use_moe', default=0, type=int, choices=[0, 1], help="æ˜¯å¦ä½¿ç”¨MoEæ¶æ„ï¼ˆ0=å¦ï¼Œ1=æ˜¯ï¼‰")
    parser.add_argument('--inference_rope_scaling', default=False, action='store_true', help="å¯ç”¨RoPEä½ç½®ç¼–ç å¤–æ¨ï¼ˆ4å€ï¼Œä»…è§£å†³ä½ç½®ç¼–ç é—®é¢˜ï¼‰")
    parser.add_argument('--max_new_tokens', default=8192, type=int, help="æœ€å¤§ç”Ÿæˆé•¿åº¦ï¼ˆæ³¨æ„ï¼šå¹¶éæ¨¡å‹å®é™…é•¿æ–‡æœ¬èƒ½åŠ›ï¼‰")
    parser.add_argument('--temperature', default=0.85, type=float, help="ç”Ÿæˆæ¸©åº¦ï¼Œæ§åˆ¶éšæœºæ€§ï¼ˆ0-1ï¼Œè¶Šå¤§è¶Šéšæœºï¼‰")
    parser.add_argument('--top_p', default=0.85, type=float, help="nucleusé‡‡æ ·é˜ˆå€¼ï¼ˆ0-1ï¼‰")
    parser.add_argument('--historys', default=0, type=int, help="æºå¸¦å†å²å¯¹è¯è½®æ•°ï¼ˆéœ€ä¸ºå¶æ•°ï¼Œ0è¡¨ç¤ºä¸æºå¸¦å†å²ï¼‰")
    parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu', type=str, help="è¿è¡Œè®¾å¤‡")
   
   
   
    parser.add_argument("--test_data_path", type=str, default=DEFAULT_CONFIG["test_data_path"], help="æµ‹è¯•æ•°æ®JSONè·¯å¾„")
    parser.add_argument("--output_report_path", type=str, default=DEFAULT_CONFIG["output_report_path"], help="è¾“å‡ºJSONæŠ¥å‘Šè·¯å¾„")
   
   
    args = parser.parse_args()

    # åˆå¹¶é…ç½®
    run_config = DEFAULT_CONFIG.copy()
    run_config.update(vars(args))

    try:
        # æ‰§è¡Œæ‰¹é‡æµ‹è¯•
        test_results = run_batch_test(run_config,args)

        # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
        generate_human_report(test_results, run_config)

        # æ‰“å°æ±‡æ€»ç»“æœ
        print("\n" + "="*50)
        print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
        print("="*50)
        print(f"æ€»é¢˜ç›®æ•°ï¼š{test_results['total_questions']}")
        print(f"æ­£ç¡®æ•°ï¼š{test_results['correct_count']}")
        print(f"é”™è¯¯æ•°ï¼š{test_results['incorrect_count']}")
        print(f"æ•´ä½“å‡†ç¡®ç‡ï¼š{test_results['accuracy']}%")
        print("="*50)

    except Exception as e:
        print(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥ï¼š{str(e)}")
        import traceback
        traceback.print_exc()